model_list:
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-pro-vertex
    litellm_params:
      model: vertex_ai/gemini-2.5-pro
      vertex_project: "crested-archive-459519-e3"
      vertex_location: "global"
      vertex_region: "global"
      vertex_credentials: /app/key.json
    model_info:
      base_model: gemini/gemini-2.5-pro

  - model_name: gemini-2.5-pro-openrouter
    litellm_params:
      model: openrouter/google/gemini-2.5-pro
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-flash-vertex
    litellm_params:
      model: vertex_ai/gemini-2.5-flash
      vertex_project: "crested-archive-459519-e3"
      vertex_location: "global"
      vertex_region: "global"
      vertex_credentials: /app/key.json
    model_info:
      base_model: gemini/gemini-2.5-flash

  - model_name: gemini-2.5-flash-openrouter
    litellm_params:
      model: openrouter/google/gemini-2.5-flash
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  - model_name: gemini-2.5-flash-search
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY
      web_search_options:
        search_context_size: "high"
      tools:
        - urlContext: {}

  - model_name: gemini-2.5-flash-search-vertex
    litellm_params:
      model: vertex_ai/gemini-2.5-flash
      vertex_project: "crested-archive-459519-e3"
      vertex_location: "global"
      vertex_region: "global"
      vertex_credentials: /app/key.json
      web_search_options:
        search_context_size: "high"
    model_info:
      base_model: gemini/gemini-2.5-flash

  - model_name: gemini-2.5-flash-lite
    litellm_params:
      model: vertex_ai/gemini-2.5-flash-lite
      vertex_project: "crested-archive-459519-e3"
      vertex_location: "global"
      vertex_region: "global"
      vertex_credentials: /app/key.json

  - model_name: gemini-2.5-flash-lite-aistudio
    litellm_params:
      model: gemini/gemini-2.5-flash-lite
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-flash-lite-openrouter
    litellm_params:
      model: openrouter/google/gemini-2.5-flash-lite
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  # ---------- xAI / Grok ----------
  - model_name: grok-4
    litellm_params:
      model: xai/grok-4
      api_key: os.environ/XAI_API_KEY
      drop_params: true
      additional_drop_params: ["reasoning_effort"]
    model_info:
      mode: "chat"
      # xAI published context/pricing; higher-context (>128k) costs double
      max_input_tokens: 256000
      max_output_tokens: 128000
      input_cost_per_token: 3e-06            # $3.00 / 1M
      output_cost_per_token: 1.5e-05         # $15.00 / 1M
      input_cost_per_token_above_128k_tokens: 6e-06
      output_cost_per_token_above_128k_tokens: 3e-05
      supports_reasoning: true

  - model_name: grok-4-fast-openrouter
    litellm_params:
      model: openrouter/x-ai/grok-4-fast:free
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  - model_name: grok-4-fast
    litellm_params:
      model: xai/grok-4-fast
      api_key: os.environ/XAI_API_KEY
    model_info:
      mode: "chat"
      # 2M context; explicit tiered cost since xAI API doesn't return cost
      max_input_tokens: 2000000
      max_output_tokens: 30000
      input_cost_per_token: 2e-07            # <$128k
      output_cost_per_token: 5e-07
      input_cost_per_token_above_128k_tokens: 4e-07
      output_cost_per_token_above_128k_tokens: 1e-06
      supports_reasoning: true

  - model_name: grok-4-fast-search
    litellm_params:
      model: xai/grok-4-fast
      api_key: os.environ/XAI_API_KEY
      web_search_options:
        search_context_size: "high"
    model_info:
      # inherit the same tiered costs as grok-4-fast above for proper tracking
      input_cost_per_token: 2e-07
      output_cost_per_token: 5e-07
      input_cost_per_token_above_128k_tokens: 4e-07
      output_cost_per_token_above_128k_tokens: 1e-06

  # ---------- OpenAI ----------
  - model_name: o3
    litellm_params:
      model: openai/o3
      api_key: os.environ/OPENAI_API_KEY
      extra_body:
        service_tier: "flex"
    # Manual 50% flex override
    model_info:
      mode: "chat"
      max_input_tokens: 200000
      max_output_tokens: 100000
      input_cost_per_token: 1e-06           # flex = half of 2e-06
      output_cost_per_token: 4e-06          # flex = half of 8e-06

  - model_name: gpt-5
    litellm_params:
      model: openai/gpt-5
      api_key: os.environ/OPENAI_API_KEY
      extra_body:
        service_tier: "flex"
    # Manual 50% flex override
    model_info:
      mode: "chat"
      max_input_tokens: 400000
      max_output_tokens: 128000
      input_cost_per_token: 6.25e-07        # flex = half of 1.25e-06
      output_cost_per_token: 5e-06          # flex = half of 1e-05

  - model_name: gpt-5-mini
    litellm_params:
      model: openai/gpt-5-mini
      api_key: os.environ/OPENAI_API_KEY
      extra_body:
        service_tier: "flex"
    # Manual 50% flex override
    model_info:
      mode: "chat"
      max_input_tokens: 400000
      max_output_tokens: 128000
      input_cost_per_token: 1.25e-07        # flex = half of 2.5e-07
      output_cost_per_token: 1e-06          # flex = half of 2e-06

  - model_name: gpt-4o-search
    litellm_params:
      model: openai/gpt-4o-search-preview
      api_key: os.environ/OPENAI_API_KEY
      web_search_options:
        search_context_size: "high"

  - model_name: sonar-pro-search
    litellm_params:
      model: openrouter/perplexity/sonar-pro
      api_key: os.environ/OPENROUTER_API_KEY
      web_search_options:
        search_context_size: "high"
      extra_body:
        usage:
          include: true

  - model_name: claude-sonnet-4
    litellm_params:
      model: openrouter/anthropic/claude-sonnet-4
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  - model_name: qwen3-235b-a22b
    litellm_params:
      model: openrouter/qwen/qwen3-235b-a22b-thinking-2507
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  - model_name: glm-4.5
    litellm_params:
      model: openrouter/z-ai/glm-4.5
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

  - model_name: kimi-k2
    litellm_params:
      model: openrouter/moonshotai/kimi-k2-0905
      api_key: os.environ/OPENROUTER_API_KEY
      extra_body:
        usage:
          include: true

# ---------- Router Settings ----------
router_settings:
  routing_strategy: simple-shuffle
  model_group_alias:
    large-model: ["gemini-2.5-pro"]
    small-model: ["gemini-2.5-flash"]
  fallbacks:
    - {"gemini-2.5-flash-search": ["gemini-2.5-flash-search-vertex"]}
    - {"gemini-2.5-flash": ["gemini-2.5-flash-vertex", "gemini-2.5-flash-openrouter", "grok-4-fast"]}
    - {"gemini-2.5-pro": ["gemini-2.5-pro-vertex", "gemini-2.5-pro-openrouter", "gpt-5", "grok-4"]}
    - {"gemini-2.5-flash-lite": ["gemini-2.5-flash-lite-aistudio", "gemini-2.5-flash-lite-openrouter"]}
    - {"gpt-5": ["gemini-2.5-pro"]}
    - {"gpt-5-mini": ["gemini-2.5-flash"]}
    - {"grok-4-fast": ["gemini-2.5-flash"]}

# ---------- LiteLLM Settings ----------
litellm_settings:
  request_timeout: 1800
  num_retries: 3
  retry_after: 15
  telemetry: false
  cache: true
  cache_params:
    type: "redis"
    host: "redis"
    port: 6379
    ttl: 86400
  drop_params: true
  extra_spend_tag_headers:
    - "x-litellm-spend"

# ---------- General Settings ----------
general_settings:
  database_url: os.environ/LITELLM_DATABASE_URL
  store_prompts_in_spend_logs: true
  master_key: os.environ/LITELLM_MASTER_KEY
  store_model_in_db: true
