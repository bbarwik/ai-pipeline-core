# Pipeline Core Configuration
# Copy this file to .env and fill in your actual values

# LLM API Configuration
# OpenAI-compatible API endpoint (e.g., for local LiteLLM proxy)
OPENAI_BASE_URL=http://localhost:4000/v1
# Your OpenAI API key or LiteLLM proxy key
OPENAI_API_KEY=your-api-key-here

# Prefect Configuration (optional)
# Prefect Cloud API URL (leave empty for local Prefect)
PREFECT_API_URL=
# Prefect Cloud API key (only needed for Prefect Cloud)
PREFECT_API_KEY=

# Observability (optional)
# Laminar project API key for LLM observability
LMNR_PROJECT_API_KEY=
# Enable debug mode for Laminar tracing
LMNR_DEBUG=

# Storage Configuration (optional)
# Prefect GcsBucket block name for Google Cloud Storage access
GCS_BLOCK=
# Default GCS bucket name (optional)
GCS_BUCKET=

# Additional environment variables for debugging (optional)
# Set session ID for tracing
# LMNR_SESSION_ID=your-session-id
# Set user ID for tracing
# LMNR_USER_ID=your-user-id
