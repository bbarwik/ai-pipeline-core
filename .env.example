# Pipeline Core Configuration
# Copy this file to .env and fill in your actual values

# LLM API Configuration
# OpenAI-compatible API endpoint (e.g., for local LiteLLM proxy)
OPENAI_BASE_URL=http://localhost:4000/v1
# Your OpenAI API key or LiteLLM proxy key
OPENAI_API_KEY=your-api-key-here

# Prefect Configuration (optional)
# Prefect Cloud API URL (leave empty for local Prefect)
PREFECT_API_URL=
# Prefect Cloud API key (only needed for Prefect Cloud)
PREFECT_API_KEY=
PREFECT_API_AUTH_STRING=
PREFECT_WORK_POOL_NAME=default
PREFECT_WORK_QUEUE_NAME=default
PREFECT_GCS_BUCKET=

# Observability (optional)
# Laminar project API key for LLM observability
LMNR_PROJECT_API_KEY=
# Enable debug mode for Laminar tracing
LMNR_DEBUG=

# Storage Configuration (optional)
# Path to GCS service account JSON file for authentication
GCS_SERVICE_ACCOUNT_FILE=

# ClickHouse (optional — document storage + tracking)
CLICKHOUSE_HOST=
CLICKHOUSE_PORT=8443
CLICKHOUSE_DATABASE=default
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=
CLICKHOUSE_SECURE=true

# Tracking behavior (optional)
TRACKING_ENABLED=true
TRACKING_SUMMARY_MODEL=gemini-3-flash

# Document summaries — store-level, LLM-generated (optional)
DOC_SUMMARY_ENABLED=true
DOC_SUMMARY_MODEL=gemini-3-flash
